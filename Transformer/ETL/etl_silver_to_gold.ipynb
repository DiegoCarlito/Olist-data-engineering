{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a63743a9",
   "metadata": {},
   "source": [
    "# Pipeline ETL: Camada Prata para Ouro\n",
    "\n",
    "## 1. Objetivo\n",
    "\n",
    "Este notebook é responsável pela construção do Data Warehouse (Camada Gold). O processo consiste em:\n",
    "1.  **Extrair** os dados denormalizados da tabela `orders` (Camada Silver).\n",
    "2.  **Transformar** os dados aplicando a modelagem dimensional (Star Schema), separando os atributos em tabelas Dimensão e as métricas em uma tabela Fato.\n",
    "3.  **Carregar** os dados no schema `dw` do PostgreSQL, gerando chaves substitutas (Surrogate Keys) para garantir a integridade referencial e performance de BI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETLSilverToGoldOlist\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.5.0\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"SparkSession iniciada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e8ed41",
   "metadata": {},
   "source": [
    "## 2. Preparação do Ambiente (Schema DW)\n",
    "\n",
    "Antes de iniciar a carga, executamos o script DDL para garantir que o schema `dw` e as tabelas de destino existam e estejam limpas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7104d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando DDL...\n",
      "Schema DW recriado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "def carregar_env():\n",
    "    env_path = '../../.env'\n",
    "    load_dotenv(dotenv_path=env_path)\n",
    "    return {\n",
    "        \"user\": os.getenv(\"DB_USER\"),\n",
    "        \"password\": os.getenv(\"DB_PASSWORD\"),\n",
    "        \"host\": os.getenv(\"DB_HOST\"),\n",
    "        \"port\": os.getenv(\"DB_PORT\"),\n",
    "        \"dbname\": os.getenv(\"DB_NAME\")\n",
    "    }\n",
    "\n",
    "def resetar_dw(env_vars):\n",
    "    ddl_path = \"../../DataLayer/gold/ddl.sql\"\n",
    "    print(\"Executando DDL...\")\n",
    "    try:\n",
    "        conn = psycopg2.connect(**env_vars)\n",
    "        conn.autocommit = True\n",
    "        cur = conn.cursor()\n",
    "        with open(ddl_path, 'r') as f:\n",
    "            cur.execute(f.read())\n",
    "        print(\"Schema DW recriado com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no DDL: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cur' in locals(): cur.close()\n",
    "        if 'conn' in locals(): conn.close()\n",
    "\n",
    "env_vars = carregar_env()\n",
    "resetar_dw(env_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395af1da",
   "metadata": {},
   "source": [
    "## 3. Extração (Leitura da Silver)\n",
    "\n",
    "Lemos os dados da tabela `public.orders` para um DataFrame Spark. Esta é a nossa fonte única da verdade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a46b7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo dados da camada Silver...\n",
      "Linhas carregadas: 88981\n"
     ]
    }
   ],
   "source": [
    "jdbc_url = f\"jdbc:postgresql://{env_vars['host']}:{env_vars['port']}/{env_vars['dbname']}\"\n",
    "jdbc_props = {\"user\": env_vars['user'], \"password\": env_vars['password'], \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "print(\"Lendo dados da camada Silver...\")\n",
    "df_silver = spark.read.jdbc(jdbc_url, \"public.orders\", properties=jdbc_props)\n",
    "df_silver.cache()\n",
    "print(f\"Linhas carregadas: {df_silver.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7658ea",
   "metadata": {},
   "source": [
    "## 4. Processamento das Dimensões (Transform & Load)\n",
    "\n",
    "Nesta etapa, \"fatiamos\" os dados da Silver para criar as tabelas de dimensão. Para cada dimensão:\n",
    "1.  Selecionamos colunas distintas.\n",
    "2.  Geramos uma Surrogate Key sequencial.\n",
    "3.  Carregamos na tabela correspondente no PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b34bb64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando e Carregando Dimensões Cliente, Vendedor, Produto, Pagamento...\n",
      "Carga de Dimensões concluída.\n"
     ]
    }
   ],
   "source": [
    "print(\"Processando e Carregando Dimensões Cliente, Vendedor, Produto, Pagamento...\")\n",
    "\n",
    "dim_cli = df_silver.select(\n",
    "    F.col(\"customer_unique_id\").alias(\"ntk_idn_cli\"),\n",
    "    F.col(\"customer_city\").alias(\"nom_cid\"),\n",
    "    F.col(\"customer_state\").alias(\"sig_est\")\n",
    ").distinct()\n",
    "dim_cli.write.jdbc(jdbc_url, \"dw.dim_cli\", \"append\", jdbc_props)\n",
    "\n",
    "dim_vnd = df_silver.select(\n",
    "    F.col(\"seller_id\").alias(\"ntk_idn_vnd\"),\n",
    "    F.col(\"seller_city\").alias(\"nom_cid\"),\n",
    "    F.col(\"seller_state\").alias(\"sig_est\")\n",
    ").distinct()\n",
    "dim_vnd.write.jdbc(jdbc_url, \"dw.dim_vnd\", \"append\", jdbc_props)\n",
    "\n",
    "dim_pro = df_silver.select(\n",
    "    F.col(\"product_id\").alias(\"ntk_idn_pro\"),\n",
    "    F.col(\"product_category_name\").alias(\"nom_cat\")\n",
    ").distinct()\n",
    "dim_pro.write.jdbc(jdbc_url, \"dw.dim_pro\", \"append\", jdbc_props)\n",
    "\n",
    "dim_pag = df_silver.select(\n",
    "    F.col(\"payment_type\").alias(\"dsc_tip_pag\"),\n",
    "    F.col(\"payment_installments\").alias(\"num_par\")\n",
    ").distinct()\n",
    "dim_pag.write.jdbc(jdbc_url, \"dw.dim_pag\", \"append\", jdbc_props)\n",
    "\n",
    "print(\"Carga de Dimensões concluída.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31113e73",
   "metadata": {},
   "source": [
    "### 4.1. Dimensão Tempo\n",
    "\n",
    "A Dimensão Tempo é gerada a partir das datas de compra existentes. Enriquecemos os dados extraindo atributos como Dia, Mês, Ano, Trimestre, Semestre e Flag de Fim de Semana para facilitar a análise temporal no dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1defa81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando e Carregando Dimensão Tempo...\n",
      "Dimensão Tempo carregada.\n"
     ]
    }
   ],
   "source": [
    "print(\"Processando e Carregando Dimensão Tempo...\")\n",
    "\n",
    "df_datas = df_silver.select(F.to_date(\"order_purchase_timestamp\").alias(\"dat_cmp\")).distinct()\n",
    "\n",
    "dim_tmp = df_datas.filter(F.col(\"dat_cmp\").isNotNull()).select(\n",
    "    F.col(\"dat_cmp\"),\n",
    "    F.date_format(\"dat_cmp\", \"yyyyMMdd\").cast(\"int\").alias(\"srk_tmp\"),\n",
    "    F.year(\"dat_cmp\").alias(\"num_ano\"),\n",
    "    F.month(\"dat_cmp\").alias(\"num_mes\"),\n",
    "    F.date_format(\"dat_cmp\", \"MMMM\").alias(\"nom_mes\"),\n",
    "    F.dayofmonth(\"dat_cmp\").alias(\"num_dia\"),\n",
    "    F.quarter(\"dat_cmp\").alias(\"num_tri\"),\n",
    "    F.date_format(\"dat_cmp\", \"EEEE\").alias(\"nom_dia_sem\"),\n",
    "    (F.dayofweek(\"dat_cmp\").isin([1, 7])).alias(\"flg_fds\")\n",
    ")\n",
    "\n",
    "dim_tmp.write.jdbc(jdbc_url, \"dw.dim_tmp\", \"append\", jdbc_props)\n",
    "print(\"Dimensão Tempo carregada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83290cee",
   "metadata": {},
   "source": [
    "## 5. Processamento da Tabela Fato (Transform & Load)\n",
    "\n",
    "Esta é a etapa final de integração. Realizamos o **JOIN** da tabela Silver original com as novas Dimensões criadas para substituir as chaves naturais (IDs originais) pelas chaves artificiais do DW.\n",
    "\n",
    "Selecionamos apenas as chaves estrangeiras e as métricas numéricas para compor a tabela `dw.fat_ped`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc29a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando Tabela Fato...\n",
      "Lendo dimensões do DW (com SKs)...\n",
      "Fazendo JOINs da Fato com as Dimensões...\n",
      "Selecionando colunas finais da Fato...\n",
      "Carregando Tabela Fato...\n",
      "Carga da Camada Gold concluída com sucesso.\n"
     ]
    }
   ],
   "source": [
    "print(\"Processando Tabela Fato...\")\n",
    "print(\"Lendo dimensões do DW (com SKs)...\")\n",
    "\n",
    "dim_cli_sk = spark.read.jdbc(jdbc_url, \"dw.dim_cli\", properties=jdbc_props)\n",
    "dim_vnd_sk = spark.read.jdbc(jdbc_url, \"dw.dim_vnd\", properties=jdbc_props)\n",
    "dim_pro_sk = spark.read.jdbc(jdbc_url, \"dw.dim_pro\", properties=jdbc_props)\n",
    "dim_pag_sk = spark.read.jdbc(jdbc_url, \"dw.dim_pag\", properties=jdbc_props)\n",
    "dim_tmp_sk = spark.read.jdbc(jdbc_url, \"dw.dim_tmp\", properties=jdbc_props)\n",
    "\n",
    "df_fato_base = df_silver.withColumn(\"data_join\", F.to_date(\"order_purchase_timestamp\"))\n",
    "\n",
    "print(\"Fazendo JOINs da Fato com as Dimensões...\")\n",
    "df_fato = df_fato_base \\\n",
    "    .join(dim_cli_sk, df_fato_base.customer_unique_id == dim_cli_sk.ntk_idn_cli, \"left\") \\\n",
    "    .join(dim_vnd_sk, df_fato_base.seller_id == dim_vnd_sk.ntk_idn_vnd, \"left\") \\\n",
    "    .join(dim_pro_sk, df_fato_base.product_id == dim_pro_sk.ntk_idn_pro, \"left\") \\\n",
    "    .join(dim_tmp_sk, df_fato_base.data_join == dim_tmp_sk.dat_cmp, \"left\") \\\n",
    "    .join(dim_pag_sk, \n",
    "          (df_fato_base.payment_type == dim_pag_sk.dsc_tip_pag) & \n",
    "          (df_fato_base.payment_installments == dim_pag_sk.num_par), \"left\")\n",
    "\n",
    "print(\"Selecionando colunas finais da Fato...\")\n",
    "fat_ped = df_fato.select(\n",
    "    F.col(\"srk_cli\"),\n",
    "    F.col(\"srk_vnd\"),\n",
    "    F.col(\"srk_pro\"),\n",
    "    F.col(\"srk_tmp\"),\n",
    "    F.col(\"srk_pag\"),\n",
    "    F.col(\"order_id\").alias(\"ntk_idn_ped\"),\n",
    "    (F.col(\"price\") + F.col(\"freight_value\")).alias(\"vlr_tot\"),\n",
    "    F.col(\"freight_value\").alias(\"vlr_frt\"),\n",
    "    F.col(\"price\").alias(\"vlr_itm\"),\n",
    "    F.col(\"delivery_days\").alias(\"qtd_dia_ent\"),\n",
    "    F.col(\"review_score\").alias(\"num_ava\"),\n",
    "    F.col(\"is_delivery_late\").alias(\"flg_atr\")\n",
    ")\n",
    "\n",
    "print(\"Carregando Tabela Fato...\")\n",
    "fat_ped.write.jdbc(jdbc_url, \"dw.fat_ped\", \"append\", jdbc_props)\n",
    "print(\"Carga da Camada Gold concluída com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "597464f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
