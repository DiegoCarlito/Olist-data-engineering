{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ead078c",
   "metadata": {},
   "source": [
    "# Pipeline de ETL: Camada Bronze para Prata\n",
    "\n",
    "## 1. Objetivo\n",
    "\n",
    "Este notebook é o motor central do pipeline de ETL. Ele é responsável por:\n",
    "1.  **Extrair (Extract)**: Ler todos os dados brutos da Camada Bronze (`/DataLayer/raw/data.zip`).\n",
    "2.  **Transformar (Transform)**: Aplicar todas as regras de negócio descobertas na análise, incluindo:\n",
    "    * Unificação (JOINs) de todas as tabelas.\n",
    "    * **Tratamento de Outliers** (via método IQR), conforme feedback.\n",
    "    * Limpeza de tipos de dados (ex: `try_cast` em `review_score`).\n",
    "    * Imputação de nulos (ex: `coalesce` em `categoria_produto`).\n",
    "    * Engenharia de Features (ex: `tempo_entrega_dias`).\n",
    "3.  **Carregar (Load)**: Salvar o resultado limpo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "import zipfile \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETLBronzeParaPrataOlist\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.5.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession iniciada com o driver PostgreSQL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f7ac17",
   "metadata": {},
   "source": [
    "## 2. Extração (Extract)\n",
    "\n",
    "Carregamos todos os datasets da camada Bronze (`/DataLayer/raw/data.zip`) para DataFrames Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ff9b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraindo '../../DataLayer/raw/data.zip' para './data/'...\n",
      "Arquivos extraídos com sucesso.\n",
      "\n",
      "Carregando arquivos CSV para DataFrames PySpark...\n",
      "- DataFrame 'olist_customers' carregado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- DataFrame 'olist_geolocation' carregado.\n",
      "- DataFrame 'olist_order_items' carregado.\n",
      "- DataFrame 'olist_order_payments' carregado.\n",
      "- DataFrame 'olist_order_reviews' carregado.\n",
      "- DataFrame 'olist_orders' carregado.\n",
      "- DataFrame 'olist_products' carregado.\n",
      "- DataFrame 'olist_sellers' carregado.\n",
      "- DataFrame 'product_category_name_translation' carregado.\n",
      "\n",
      "DataFrames disponíveis para análise:\n",
      "- olist_customers\n",
      "- olist_geolocation\n",
      "- olist_order_items\n",
      "- olist_order_payments\n",
      "- olist_order_reviews\n",
      "- olist_orders\n",
      "- olist_products\n",
      "- olist_sellers\n",
      "- product_category_name_translation\n"
     ]
    }
   ],
   "source": [
    "zip_path = '../../DataLayer/raw/data.zip' \n",
    "\n",
    "extract_path = './data/' \n",
    "\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "print(f\"Extraindo '{zip_path}' para '{extract_path}'...\")\n",
    "try:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"Arquivos extraídos com sucesso.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"!!! ERRO: Arquivo '{zip_path}' não encontrado.\")\n",
    "    spark.stop()\n",
    "except Exception as e:\n",
    "    print(f\"!!! ERRO ao extrair o arquivo zip: {e} !!!\")\n",
    "    spark.stop()\n",
    "\n",
    "caminho_base_dados = extract_path \n",
    "dfs = {}\n",
    "\n",
    "arquivos = [\n",
    "    \"olist_customers_dataset.csv\", \"olist_geolocation_dataset.csv\", \n",
    "    \"olist_order_items_dataset.csv\", \"olist_order_payments_dataset.csv\",\n",
    "    \"olist_order_reviews_dataset.csv\", \"olist_orders_dataset.csv\",\n",
    "    \"olist_products_dataset.csv\", \"olist_sellers_dataset.csv\",\n",
    "    \"product_category_name_translation.csv\"\n",
    "]\n",
    "\n",
    "print(\"\\nCarregando arquivos CSV para DataFrames PySpark...\")\n",
    "for arquivo in arquivos:\n",
    "    nome_chave = arquivo.replace(\"_dataset.csv\", \"\").replace(\".csv\", \"\")\n",
    "    caminho_completo = os.path.join(caminho_base_dados, arquivo)\n",
    "    \n",
    "    if os.path.exists(caminho_completo):\n",
    "        try:\n",
    "            # Não precisamos mais do .cache() pois os arquivos não serão apagados\n",
    "            dfs[nome_chave] = spark.read.csv(caminho_completo, header=True, inferSchema=True)\n",
    "            print(f\"- DataFrame '{nome_chave}' carregado.\")\n",
    "        except Exception as e:\n",
    "            print(f\"!! ERRO ao carregar '{arquivo}': {e} !!\")\n",
    "    else:\n",
    "        print(f\"!! AVISO: Arquivo '{arquivo}' não encontrado em '{extract_path}'. Pulando...\")\n",
    "\n",
    "print(\"\\nDataFrames disponíveis para análise:\")\n",
    "for nome in dfs.keys():\n",
    "    print(f\"- {nome}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88fe22",
   "metadata": {},
   "source": [
    "## 3. Transformação (Transform)\n",
    "\n",
    "Aplicamos a unificação dos dados, seguida das regras de negócio de limpeza e enriquecimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd5aefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando unificação dos DataFrames (JOINs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/12 23:09:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 32:===============================================>      (176 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOINs concluídos. Total de linhas pós-join: 112650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando unificação dos DataFrames (JOINs)...\")\n",
    "\n",
    "df_prata = dfs[\"olist_order_items\"].join(\n",
    "    dfs[\"olist_orders\"], \"order_id\", \"left\"\n",
    ").join(\n",
    "    dfs[\"olist_products\"], \"product_id\", \"left\"\n",
    ").join(\n",
    "    dfs[\"olist_sellers\"], \"seller_id\", \"left\"\n",
    ").join(\n",
    "    dfs[\"olist_customers\"], \"customer_id\", \"left\"\n",
    ").join(\n",
    "    dfs[\"olist_order_payments\"].groupBy(\"order_id\")\n",
    "        .agg(\n",
    "            F.first(\"payment_type\").alias(\"payment_type\"),\n",
    "            F.first(\"payment_installments\").alias(\"payment_installments\"),\n",
    "            F.sum(\"payment_value\").alias(\"payment_value\")\n",
    "        ), \"order_id\", \"left\"\n",
    ").join(\n",
    "    dfs[\"olist_order_reviews\"].groupBy(\"order_id\")\n",
    "        .agg(\n",
    "            F.first(\"review_score\").alias(\"review_score\")\n",
    "        ), \"order_id\", \"left\"\n",
    ").join(\n",
    "    dfs[\"product_category_name_translation\"], \"product_category_name\", \"left\"\n",
    ")\n",
    "\n",
    "df_prata.cache()\n",
    "print(f\"JOINs concluídos. Total de linhas pós-join: {df_prata.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a509c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando tratamento de outliers (Método IQR)...\n",
      "Tratando outliers da coluna: 'price'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tratando outliers da coluna: 'freight_value'\n",
      "Tratando outliers da coluna: 'payment_value'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linhas antes do tratamento de outliers: 112,650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:============================================>         (166 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas após o tratamento de outliers: 88,981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando tratamento de outliers (Método IQR)...\")\n",
    "\n",
    "colunas_outliers = ['price', 'freight_value', 'payment_value']\n",
    "df_prata_sem_outliers = df_prata\n",
    "\n",
    "for coluna in colunas_outliers:\n",
    "    if coluna in df_prata.columns:\n",
    "        print(f\"Tratando outliers da coluna: '{coluna}'\")\n",
    "        \n",
    "        quantiles = df_prata_sem_outliers.approxQuantile(coluna, [0.25, 0.75], 0.01)\n",
    "        q1 = quantiles[0]\n",
    "        q3 = quantiles[1]\n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        limite_inferior = q1 - (1.5 * iqr)\n",
    "        limite_superior = q3 + (1.5 * iqr)\n",
    "        \n",
    "        df_prata_sem_outliers = df_prata_sem_outliers.filter(\n",
    "            (F.col(coluna) >= limite_inferior) & (F.col(coluna) <= limite_superior)\n",
    "        )\n",
    "    else:\n",
    "        print(f\"AVISO: Coluna '{coluna}' não encontrada para tratamento de outlier.\")\n",
    "\n",
    "df_prata_sem_outliers.cache()\n",
    "\n",
    "print(f\"\\nLinhas antes do tratamento de outliers: {df_prata.count():,}\")\n",
    "print(f\"Linhas após o tratamento de outliers: {df_prata_sem_outliers.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1313cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando limpeza final, seleção e engenharia de features...\n",
      "\n",
      "Transformações concluídas. Schema final da camada Prata:\n",
      "root\n",
      " |-- order_item_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- price: decimal(10,2) (nullable = true)\n",
      " |-- freight_value: decimal(10,2) (nullable = true)\n",
      " |-- product_category_name: string (nullable = false)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- seller_city: string (nullable = true)\n",
      " |-- seller_state: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- payment_installments: integer (nullable = true)\n",
      " |-- payment_value: decimal(10,2) (nullable = true)\n",
      " |-- review_score: integer (nullable = true)\n",
      " |-- delivery_days: integer (nullable = true)\n",
      " |-- is_delivery_late: boolean (nullable = true)\n",
      "\n",
      "\n",
      "Amostra dos dados transformados:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:==============================================>       (174 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+------+-------------+---------------------+---------------------+--------------+---------------+------------+------------+--------------------+-------------+------------+-------------+----------------+\n",
      "|order_item_id                     |order_id                        |customer_unique_id              |product_id                      |seller_id                       |order_status|order_purchase_timestamp|order_approved_at  |order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|price |freight_value|product_category_name|customer_city        |customer_state|seller_city    |seller_state|payment_type|payment_installments|payment_value|review_score|delivery_days|is_delivery_late|\n",
      "+----------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+------+-------------+---------------------+---------------------+--------------+---------------+------------+------------+--------------------+-------------+------------+-------------+----------------+\n",
      "|014405982914c2cde2796ddcf0b8703d_1|014405982914c2cde2796ddcf0b8703d|e8d87ee946600f7753579a074fbd2d5d|6782d593f63105318f46bbf7633279bf|325f3178fb58e2a9778334621eecdbf9|delivered   |2017-07-26 17:38:47     |2017-07-26 17:50:17|2017-07-27 19:39:52         |2017-07-31 15:53:33          |2017-08-17 00:00:00          |27.90 |3.81         |perfumery            |mesquita             |RJ            |taboao da serra|SP          |credit_card |7                   |78.43        |5           |5            |false           |\n",
      "|014405982914c2cde2796ddcf0b8703d_2|014405982914c2cde2796ddcf0b8703d|e8d87ee946600f7753579a074fbd2d5d|e95ee6822b66ac6058e2e4aff656071a|a17f621c590ea0fab3d5d883e1630ec6|delivered   |2017-07-26 17:38:47     |2017-07-26 17:50:17|2017-07-27 19:39:52         |2017-07-31 15:53:33          |2017-08-17 00:00:00          |21.33 |25.39        |sports_leisure       |mesquita             |RJ            |sorocaba       |SP          |credit_card |7                   |78.43        |5           |5            |false           |\n",
      "|019886de8f385a39b75bedbb726fd4ef_1|019886de8f385a39b75bedbb726fd4ef|d29ede26cd3e2817b314005a88bd28a8|e9a69340883a438c3f91739d14d3a56d|1b4c3a6f53068f0b6944d2d005c9fc89|delivered   |2018-02-10 12:52:51     |2018-02-10 13:08:12|2018-02-14 15:28:51         |2018-02-23 02:03:03          |2018-03-14 00:00:00          |159.90|28.50        |housewares           |campo grande         |MS            |sao ludgero    |SC          |credit_card |2                   |188.40       |5           |13           |false           |\n",
      "|01a6ad782455876aa89081449d49c452_1|01a6ad782455876aa89081449d49c452|31c23262d79bc7e803884e37f9bc5359|036734b5a58d5d4f46b0616ddc047ced|ea8482cd71df3c1969d7b9473ff13abc|delivered   |2018-01-18 10:07:52     |2018-01-18 10:17:29|2018-01-22 22:37:04         |2018-02-01 21:02:22          |2018-02-20 00:00:00          |34.99 |15.10        |telephony            |santo angelo         |RS            |sao paulo      |SP          |credit_card |5                   |50.09        |3           |14           |false           |\n",
      "|01d907b3e209269e120a365fc2b97524_1|01d907b3e209269e120a365fc2b97524|a7b781410bcc8bbf3221f48ff45aae6d|b1434a8f79cb3528540d9b21e686e823|d1c281d3ae149232351cd8c8cc885f0d|delivered   |2017-08-09 16:21:06     |2017-08-10 10:25:08|2017-08-11 19:05:53         |2017-08-16 22:34:11          |2017-08-29 00:00:00          |151.99|17.77        |bed_bath_table       |ferraz de vasconcelos|SP            |ibitinga       |SP          |credit_card |10                  |169.76       |5           |7            |false           |\n",
      "+----------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+------+-------------+---------------------+---------------------+--------------+---------------+------------+------------+--------------------+-------------+------------+-------------+----------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando limpeza final, seleção e engenharia de features...\")\n",
    "\n",
    "df_prata_transformado = df_prata_sem_outliers.select(\n",
    "    # --- CHAVES ---\n",
    "    F.concat(F.col(\"order_id\"), F.lit(\"_\"), F.col(\"order_item_id\")).alias(\"order_item_id\"), \n",
    "    F.col(\"order_id\"),\n",
    "    F.col(\"customer_unique_id\"),\n",
    "    F.col(\"product_id\"),\n",
    "    F.col(\"seller_id\"),\n",
    "    \n",
    "    # --- DADOS DO PEDIDO ---\n",
    "    F.col(\"order_status\"),\n",
    "    F.col(\"order_purchase_timestamp\"),\n",
    "    F.col(\"order_approved_at\"),\n",
    "    F.col(\"order_delivered_carrier_date\"),\n",
    "    F.col(\"order_delivered_customer_date\"),\n",
    "    F.col(\"order_estimated_delivery_date\"),\n",
    "\n",
    "    # --- DADOS DO ITEM ---\n",
    "    F.col(\"price\").cast(\"decimal(10,2)\"),\n",
    "    F.col(\"freight_value\").cast(\"decimal(10,2)\"),\n",
    "    \n",
    "    # --- DADOS DO PRODUTO ---\n",
    "    F.coalesce(F.col(\"product_category_name_english\"), F.lit(\"unknown\")).alias(\"product_category_name\"),\n",
    "\n",
    "    # --- DADOS DE LOCALIZAÇÃO ---\n",
    "    F.col(\"customer_city\"),\n",
    "    F.col(\"customer_state\"),\n",
    "    F.col(\"seller_city\"),\n",
    "    F.col(\"seller_state\"),\n",
    "\n",
    "    # --- DADOS DE PAGAMENTO ---\n",
    "    F.col(\"payment_type\"),\n",
    "    F.col(\"payment_installments\"),\n",
    "    F.col(\"payment_value\").cast(\"decimal(10,2)\"),\n",
    "\n",
    "    # --- DADOS DE AVALIAÇÃO ---\n",
    "    F.expr(\"try_cast(review_score as int)\").alias(\"review_score\"),\n",
    "    \n",
    "    # --- ENGENHARIA DE FEATURES ---\n",
    "    F.datediff(F.col(\"order_delivered_customer_date\"), F.col(\"order_purchase_timestamp\")).alias(\"delivery_days\"),\n",
    "    (F.col(\"order_delivered_customer_date\") > F.col(\"order_estimated_delivery_date\")).alias(\"is_delivery_late\")\n",
    ")\n",
    "\n",
    "df_prata_final = df_prata_transformado.filter(\n",
    "    (F.col(\"review_score\").isNull()) | (F.col(\"review_score\").between(1, 5))\n",
    ")\n",
    "\n",
    "df_prata_final.cache()\n",
    "\n",
    "print(\"\\nTransformações concluídas. Schema final da camada Prata:\")\n",
    "df_prata_final.printSchema()\n",
    "\n",
    "print(\"\\nAmostra dos dados transformados:\")\n",
    "df_prata_final.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44cd281",
   "metadata": {},
   "source": [
    "## 4. Carga (Load)\n",
    "\n",
    "Com o DataFrame final transformado e limpo, carregamos os dados nos destinos da Camada Prata:\n",
    "1.  **Data Lake (Parquet):** Formato colunar otimizado para futuras análises em Spark/BI.\n",
    "2.  **Data Lake (CSV):** Conforme solicitado, um arquivo CSV único para visualização simples.\n",
    "3.  **Data Warehouse (PostgreSQL):** Carrega os dados no banco para consumo pelo dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e3388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_variaveis_ambiente():\n",
    "    env_path = '../../.env'\n",
    "    if os.path.exists(env_path):\n",
    "        load_dotenv(dotenv_path=env_path)\n",
    "        print(\"Variáveis de ambiente carregadas.\")\n",
    "        return {\n",
    "            \"db_user\": os.getenv(\"DB_USER\"), \"db_password\": os.getenv(\"DB_PASSWORD\"),\n",
    "            \"db_host\": os.getenv(\"DB_HOST\"), \"db_port\": os.getenv(\"DB_PORT\"),\n",
    "            \"db_name\": os.getenv(\"DB_NAME\")\n",
    "        }\n",
    "    else:\n",
    "        print(f\"ERRO: Arquivo .env não encontrado em '{env_path}'\")\n",
    "        return None\n",
    "\n",
    "def gerar_e_salvar_ddl(df, table_name, schema_name, ddl_path):\n",
    "    print(f\"Gerando DDL para a tabela '{schema_name}.{table_name}'...\")\n",
    "    \n",
    "    dtype_mapping = {\n",
    "        'string': 'VARCHAR(255)', 'bigint': 'BIGINT',\n",
    "        'int': 'INTEGER', 'integer': 'INTEGER',\n",
    "        'double': 'DOUBLE PRECISION', 'float': 'FLOAT',\n",
    "        'decimal(10,2)': 'NUMERIC(10, 2)',\n",
    "        'timestamp': 'TIMESTAMP', 'date': 'DATE',\n",
    "        'boolean': 'BOOLEAN'\n",
    "    }\n",
    "    \n",
    "    schema = df.dtypes\n",
    "    \n",
    "    ddl = f\"CREATE SCHEMA IF NOT EXISTS {schema_name};\\n\\n\"\n",
    "    ddl += f\"DROP TABLE IF EXISTS {schema_name}.{table_name};\\n\\n\"\n",
    "    ddl += f\"CREATE TABLE {schema_name}.{table_name} (\\n\"\n",
    "    \n",
    "    colunas_ddl = []\n",
    "    for col_name, col_type in schema:\n",
    "        sql_type = dtype_mapping.get(col_type, 'TEXT') \n",
    "        \n",
    "        not_null_cols = ['order_item_id', 'order_id', 'customer_unique_id', 'product_id', 'seller_id']\n",
    "        not_null = \" NOT NULL\" if col_name in not_null_cols else \"\"\n",
    "        \n",
    "        pk = \" PRIMARY KEY\" if col_name == 'order_item_id' else \"\"\n",
    "        \n",
    "        colunas_ddl.append(f\"    {col_name} {sql_type}{not_null}{pk}\")\n",
    "    \n",
    "    ddl += \",\\n\".join(colunas_ddl)\n",
    "    ddl += \"\\n);\"\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(ddl_path), exist_ok=True)\n",
    "        with open(ddl_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(ddl)\n",
    "        print(f\"Script DDL salvo com sucesso em: {ddl_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO ao salvar script DDL: {e}\")\n",
    "        \n",
    "    return ddl\n",
    "\n",
    "def executar_ddl_no_banco(ddl_script, env_vars):\n",
    "    print(\"Executando DDL no banco de dados...\")\n",
    "    try:\n",
    "        conn = None\n",
    "        cur = None\n",
    "        conn = psycopg2.connect(\n",
    "            user=env_vars[\"db_user\"], password=env_vars[\"db_password\"],\n",
    "            host=env_vars[\"db_host\"], port=env_vars[\"db_port\"], dbname=env_vars[\"db_name\"]\n",
    "        )\n",
    "        conn.autocommit = True\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(ddl_script)\n",
    "        print(f\"Tabela criada com sucesso no PostgreSQL.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERRO ao executar o script DDL: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if cur: cur.close()\n",
    "        if conn: conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef15433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INICIANDO ETAPA DE CARGA (LOAD) ---\n",
      "Gerando DDL para a tabela 'public.orders'...\n",
      "Script DDL salvo com sucesso em: ../../DataLayer/silver/ddl.sql\n",
      "\n",
      "Iniciando carga para o PostgreSQL...\n",
      "Variáveis de ambiente carregadas.\n",
      "Executando DDL no banco de dados...\n",
      "Tabela criada com sucesso no PostgreSQL.\n",
      "Inserindo dados na tabela 'public.orders'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carga de dados no PostgreSQL concluída com sucesso.\n",
      "\n",
      "Iniciando carga para CSV em: ../../DataLayer/silver/orders.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo CSV movido e renomeado para: ../../DataLayer/silver/orders.csv\n",
      "Pasta temporária '../../DataLayer/silver/temp_csv_output' removida.\n",
      "\n",
      "--- Pipeline ETL concluído ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- INICIANDO ETAPA DE CARGA (LOAD) ---\")\n",
    "\n",
    "schema_name = \"public\"\n",
    "table_name = \"orders\"\n",
    "ddl_output_path = f\"../../DataLayer/silver/ddl.sql\"\n",
    "csv_final_path = f\"../../DataLayer/silver/{table_name}.csv\"\n",
    "csv_temp_path = \"../../DataLayer/silver/temp_csv_output\"\n",
    "\n",
    "ddl_script = gerar_e_salvar_ddl(df_prata_final, table_name, schema_name, ddl_output_path)\n",
    "\n",
    "print(\"\\nIniciando carga para o PostgreSQL...\")\n",
    "env_vars = carregar_variaveis_ambiente()\n",
    "\n",
    "if env_vars:\n",
    "    try:\n",
    "        executar_ddl_no_banco(ddl_script, env_vars)\n",
    "        \n",
    "        jdbc_url = f\"jdbc:postgresql://{env_vars['db_host']}:{env_vars['db_port']}/{env_vars['db_name']}\"\n",
    "        jdbc_properties = {\n",
    "            \"user\": env_vars[\"db_user\"], \"password\": env_vars[\"db_password\"],\n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "        \n",
    "        print(f\"Inserindo dados na tabela '{schema_name}.{table_name}'...\")\n",
    "        df_prata_final.write.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=f\"{schema_name}.{table_name}\",\n",
    "            mode=\"overwrite\",\n",
    "            properties=jdbc_properties\n",
    "        )\n",
    "        print(\"Carga de dados no PostgreSQL concluída com sucesso.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"!!! ERRO GERAL NA CARGA PARA O BANCO: {e} !!!\")\n",
    "else:\n",
    "    print(\"ERRO: Variáveis de ambiente não carregadas. Carga no PostgreSQL abortada.\")\n",
    "\n",
    "print(f\"\\nIniciando carga para CSV em: {csv_final_path}\")\n",
    "\n",
    "try:\n",
    "    df_prata_final.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_temp_path)\n",
    "    \n",
    "    arquivo_part = glob.glob(os.path.join(csv_temp_path, \"part-*.csv\"))\n",
    "    \n",
    "    if arquivo_part:\n",
    "        shutil.move(arquivo_part[0], csv_final_path)\n",
    "        print(f\"Arquivo CSV movido e renomeado para: {csv_final_path}\")\n",
    "        \n",
    "        shutil.rmtree(csv_temp_path)\n",
    "        print(f\"Pasta temporária '{csv_temp_path}' removida.\")\n",
    "    else:\n",
    "        print(\"ERRO: Nenhum arquivo CSV 'part-' foi encontrado na pasta temporária.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao salvar e limpar o CSV: {e}\")\n",
    "\n",
    "caminho_parquet = \"../../DataLayer/silver/data_parquet\"\n",
    "if os.path.exists(caminho_parquet):\n",
    "    shutil.rmtree(caminho_parquet)\n",
    "    print(f\"Pasta Parquet antiga '{caminho_parquet}' removida.\")\n",
    "\n",
    "print(\"\\n--- Pipeline ETL concluído ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca1dca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessão Spark finalizada.\n",
      "Pasta temporária './data/' removida com sucesso.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Sessão Spark finalizada.\")\n",
    "\n",
    "pasta_temporaria = './data/'\n",
    "\n",
    "if os.path.exists(pasta_temporaria):\n",
    "    try:\n",
    "        shutil.rmtree(pasta_temporaria)\n",
    "        print(f\"Pasta temporária '{pasta_temporaria}' removida com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"AVISO: Não foi possível remover '{pasta_temporaria}': {e}\")\n",
    "else:\n",
    "    print(f\"Pasta '{pasta_temporaria}' já não existe.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
