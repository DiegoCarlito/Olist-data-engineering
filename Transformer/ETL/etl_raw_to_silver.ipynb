{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ead078c",
   "metadata": {},
   "source": [
    "# Pipeline de ETL: Camada Bronze para Prata\n",
    "\n",
    "## 1. Objetivo\n",
    "\n",
    "Este notebook é o motor central do pipeline de ETL. Ele é responsável por:\n",
    "1.  **Extrair (Extract)**: Ler todos os dados brutos da Camada Bronze (`/DataLayer/raw/`).\n",
    "2.  **Transformar (Transform)**: Aplicar todas as regras de negócio descobertas na análise, incluindo:\n",
    "    * Unificação (JOINs) de todas as tabelas.\n",
    "    * **Tratamento de Outliers**.\n",
    "    * Limpeza de tipos de dados.\n",
    "    * Imputação de nulos.\n",
    "    * Engenharia de Features.\n",
    "3.  **Carregar (Load)**: Salvar o resultado limpo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETLBronzeParaPrataOlist\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.5.0\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"SparkSession iniciada com o driver PostgreSQL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f7ac17",
   "metadata": {},
   "source": [
    "## 2. Extração (Extract)\n",
    "\n",
    "Carregamos todos os datasets da camada Bronze (`/DataLayer/raw/`) para DataFrames Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ff9b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando extração da camada Bronze em: ../../DataLayer/raw/\n",
      "- olist_customers: Carregado.\n",
      "- olist_geolocation: Carregado.\n",
      "- olist_order_items: Carregado.\n",
      "- olist_order_payments: Carregado.\n",
      "- olist_order_reviews: Carregado.\n",
      "- olist_orders: Carregado.\n",
      "- olist_products: Carregado.\n",
      "- olist_sellers: Carregado.\n",
      "- product_category_name_translation: Carregado.\n",
      "Extração concluída.\n"
     ]
    }
   ],
   "source": [
    "caminho_base = \"../../DataLayer/raw/\"\n",
    "dfs = {}\n",
    "\n",
    "arquivos = [\n",
    "    \"olist_customers_dataset.csv\", \"olist_geolocation_dataset.csv\", \n",
    "    \"olist_order_items_dataset.csv\", \"olist_order_payments_dataset.csv\",\n",
    "    \"olist_order_reviews_dataset.csv\", \"olist_orders_dataset.csv\",\n",
    "    \"olist_products_dataset.csv\", \"olist_sellers_dataset.csv\",\n",
    "    \"product_category_name_translation.csv\"\n",
    "]\n",
    "\n",
    "print(f\"Iniciando extração da camada Bronze em: {caminho_base}\")\n",
    "\n",
    "if not os.path.exists(caminho_base):\n",
    "    print(f\"!!! ERRO FATAL: Pasta de dados não encontrada: {caminho_base}\")\n",
    "else:\n",
    "    for arquivo in arquivos:\n",
    "        nome_chave = arquivo.replace(\"_dataset.csv\", \"\").replace(\".csv\", \"\")\n",
    "        caminho_completo = os.path.join(caminho_base, arquivo)\n",
    "        \n",
    "        if os.path.exists(caminho_completo):\n",
    "            try:\n",
    "                dfs[nome_chave] = spark.read.csv(caminho_completo, header=True, inferSchema=True)\n",
    "                dfs[nome_chave].cache()\n",
    "                print(f\"- {nome_chave}: Carregado.\")\n",
    "            except Exception as e:\n",
    "                print(f\"!!! ERRO em '{arquivo}': {e}\")\n",
    "        else:\n",
    "            print(f\"!!! AVISO: Arquivo '{arquivo}' não encontrado.\")\n",
    "\n",
    "print(\"Extração concluída.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88fe22",
   "metadata": {},
   "source": [
    "## 3. Transformação (Transform)\n",
    "\n",
    "Aplicamos a unificação dos dados, seguida das regras de negócio de limpeza e enriquecimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd5aefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando unificação dos DataFrames (JOINs)...\n",
      "JOINs concluídos. Total de linhas pós-join: 112650\n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando unificação dos DataFrames (JOINs)...\")\n",
    "\n",
    "df_prata = dfs[\"olist_order_items\"].join(\n",
    "    dfs[\"olist_orders\"], \"order_id\", \"left\"\n",
    ").join(\n",
    "    dfs[\"olist_products\"], \"product_id\", \"left\"\n",
    ").join(\n",
    "    dfs[\"olist_sellers\"], \"seller_id\", \"left\"\n",
    ").join(\n",
    "    dfs[\"olist_customers\"], \"customer_id\", \"left\"\n",
    ").join(\n",
    "    dfs[\"olist_order_payments\"].groupBy(\"order_id\")\n",
    "        .agg(\n",
    "            F.first(\"payment_type\").alias(\"payment_type\"),\n",
    "            F.first(\"payment_installments\").alias(\"payment_installments\"),\n",
    "            F.sum(\"payment_value\").alias(\"payment_value\")\n",
    "        ), \"order_id\", \"left\"\n",
    ").join(\n",
    "    dfs[\"olist_order_reviews\"].groupBy(\"order_id\")\n",
    "        .agg(\n",
    "            F.first(\"review_score\").alias(\"review_score\")\n",
    "        ), \"order_id\", \"left\"\n",
    ").join(\n",
    "    dfs[\"product_category_name_translation\"], \"product_category_name\", \"left\"\n",
    ")\n",
    "\n",
    "df_prata.cache()\n",
    "print(f\"JOINs concluídos. Total de linhas pós-join: {df_prata.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a509c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando tratamento de outliers (Método IQR)...\n",
      "Tratando outliers da coluna: 'price'\n",
      "Tratando outliers da coluna: 'freight_value'\n",
      "Tratando outliers da coluna: 'payment_value'\n",
      "\n",
      "Linhas antes do tratamento de outliers: 112,650\n",
      "Linhas após o tratamento de outliers: 88,981\n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando tratamento de outliers (Método IQR)...\")\n",
    "\n",
    "colunas_outliers = ['price', 'freight_value', 'payment_value']\n",
    "df_prata_sem_outliers = df_prata\n",
    "\n",
    "for coluna in colunas_outliers:\n",
    "    if coluna in df_prata.columns:\n",
    "        print(f\"Tratando outliers da coluna: '{coluna}'\")\n",
    "        \n",
    "        quantiles = df_prata_sem_outliers.approxQuantile(coluna, [0.25, 0.75], 0.01)\n",
    "        q1 = quantiles[0]\n",
    "        q3 = quantiles[1]\n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        limite_inferior = q1 - (1.5 * iqr)\n",
    "        limite_superior = q3 + (1.5 * iqr)\n",
    "        \n",
    "        df_prata_sem_outliers = df_prata_sem_outliers.filter(\n",
    "            (F.col(coluna) >= limite_inferior) & (F.col(coluna) <= limite_superior)\n",
    "        )\n",
    "    else:\n",
    "        print(f\"AVISO: Coluna '{coluna}' não encontrada para tratamento de outlier.\")\n",
    "\n",
    "df_prata_sem_outliers.cache()\n",
    "\n",
    "print(f\"\\nLinhas antes do tratamento de outliers: {df_prata.count():,}\")\n",
    "print(f\"Linhas após o tratamento de outliers: {df_prata_sem_outliers.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1313cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando limpeza final, seleção e engenharia de features...\n",
      "\n",
      "Transformações concluídas. Schema final da camada Prata:\n",
      "root\n",
      " |-- order_item_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- price: decimal(10,2) (nullable = true)\n",
      " |-- freight_value: decimal(10,2) (nullable = true)\n",
      " |-- product_category_name: string (nullable = false)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- seller_city: string (nullable = true)\n",
      " |-- seller_state: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- payment_installments: integer (nullable = true)\n",
      " |-- payment_value: decimal(10,2) (nullable = true)\n",
      " |-- review_score: integer (nullable = true)\n",
      " |-- delivery_days: integer (nullable = true)\n",
      " |-- is_delivery_late: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando limpeza final, seleção e engenharia de features...\")\n",
    "\n",
    "df_prata_transformado = df_prata_sem_outliers.select(\n",
    "    # --- CHAVES ---\n",
    "    F.concat(F.col(\"order_id\"), F.lit(\"_\"), F.col(\"order_item_id\")).alias(\"order_item_id\"), \n",
    "    F.col(\"order_id\"),\n",
    "    F.col(\"customer_unique_id\"),\n",
    "    F.col(\"product_id\"),\n",
    "    F.col(\"seller_id\"),\n",
    "    \n",
    "    # --- DADOS DO PEDIDO ---\n",
    "    F.col(\"order_status\"),\n",
    "    F.col(\"order_purchase_timestamp\"),\n",
    "    F.col(\"order_approved_at\"),\n",
    "    F.col(\"order_delivered_carrier_date\"),\n",
    "    F.col(\"order_delivered_customer_date\"),\n",
    "    F.col(\"order_estimated_delivery_date\"),\n",
    "\n",
    "    # --- DADOS DO ITEM ---\n",
    "    F.col(\"price\").cast(\"decimal(10,2)\"),\n",
    "    F.col(\"freight_value\").cast(\"decimal(10,2)\"),\n",
    "    \n",
    "    # --- DADOS DO PRODUTO ---\n",
    "    F.coalesce(F.col(\"product_category_name_english\"), F.lit(\"unknown\")).alias(\"product_category_name\"),\n",
    "\n",
    "    # --- DADOS DE LOCALIZAÇÃO ---\n",
    "    F.col(\"customer_city\"),\n",
    "    F.col(\"customer_state\"),\n",
    "    F.col(\"seller_city\"),\n",
    "    F.col(\"seller_state\"),\n",
    "\n",
    "    # --- DADOS DE PAGAMENTO ---\n",
    "    F.col(\"payment_type\"),\n",
    "    F.col(\"payment_installments\"),\n",
    "    F.col(\"payment_value\").cast(\"decimal(10,2)\"),\n",
    "\n",
    "    # --- DADOS DE AVALIAÇÃO ---\n",
    "    F.expr(\"try_cast(review_score as int)\").alias(\"review_score\"),\n",
    "    \n",
    "    # --- ENGENHARIA DE FEATURES ---\n",
    "    F.datediff(F.col(\"order_delivered_customer_date\"), F.col(\"order_purchase_timestamp\")).alias(\"delivery_days\"),\n",
    "    (F.col(\"order_delivered_customer_date\") > F.col(\"order_estimated_delivery_date\")).alias(\"is_delivery_late\")\n",
    ")\n",
    "\n",
    "df_prata_final = df_prata_transformado.filter(\n",
    "    (F.col(\"review_score\").isNull()) | (F.col(\"review_score\").between(1, 5))\n",
    ")\n",
    "\n",
    "df_prata_final.cache()\n",
    "\n",
    "print(\"\\nTransformações concluídas. Schema final da camada Prata:\")\n",
    "df_prata_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44cd281",
   "metadata": {},
   "source": [
    "## 4. Carga (Load)\n",
    "\n",
    "Com o DataFrame final transformado e limpo, carregamos os dados nos destinos da Camada Prata:\n",
    "1.  **Data Lake (Parquet):** Formato colunar otimizado para futuras análises em Spark/BI.\n",
    "2.  **Data Lake (CSV):** Conforme solicitado, um arquivo CSV único para visualização simples.\n",
    "3.  **Data Warehouse (PostgreSQL):** Carrega os dados no banco para consumo pelo dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e3388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_variaveis_ambiente():\n",
    "    env_path = '../../.env'\n",
    "    if os.path.exists(env_path):\n",
    "        load_dotenv(dotenv_path=env_path)\n",
    "        print(\"Variáveis de ambiente carregadas.\")\n",
    "        return {\n",
    "            \"db_user\": os.getenv(\"DB_USER\"), \"db_password\": os.getenv(\"DB_PASSWORD\"),\n",
    "            \"db_host\": os.getenv(\"DB_HOST\"), \"db_port\": os.getenv(\"DB_PORT\"),\n",
    "            \"db_name\": os.getenv(\"DB_NAME\")\n",
    "        }\n",
    "    else:\n",
    "        print(f\"ERRO: Arquivo .env não encontrado em '{env_path}'\")\n",
    "        return None\n",
    "\n",
    "def gerar_e_salvar_ddl(df, table_name, schema_name, ddl_path):\n",
    "    print(f\"Gerando DDL para a tabela '{schema_name}.{table_name}'...\")\n",
    "    \n",
    "    dtype_mapping = {\n",
    "        'string': 'VARCHAR(255)', 'bigint': 'BIGINT',\n",
    "        'int': 'INTEGER', 'integer': 'INTEGER',\n",
    "        'double': 'DOUBLE PRECISION', 'float': 'FLOAT',\n",
    "        'decimal(10,2)': 'NUMERIC(10, 2)',\n",
    "        'timestamp': 'TIMESTAMP', 'date': 'DATE',\n",
    "        'boolean': 'BOOLEAN'\n",
    "    }\n",
    "    \n",
    "    schema = df.dtypes\n",
    "    \n",
    "    ddl = f\"CREATE SCHEMA IF NOT EXISTS {schema_name};\\n\\n\"\n",
    "    ddl += f\"DROP TABLE IF EXISTS {schema_name}.{table_name};\\n\\n\"\n",
    "    ddl += f\"CREATE TABLE {schema_name}.{table_name} (\\n\"\n",
    "    \n",
    "    colunas_ddl = []\n",
    "    for col_name, col_type in schema:\n",
    "        sql_type = dtype_mapping.get(col_type, 'TEXT') \n",
    "        \n",
    "        not_null_cols = ['order_item_id', 'order_id', 'customer_unique_id', 'product_id', 'seller_id']\n",
    "        not_null = \" NOT NULL\" if col_name in not_null_cols else \"\"\n",
    "        \n",
    "        pk = \" PRIMARY KEY\" if col_name == 'order_item_id' else \"\"\n",
    "        \n",
    "        colunas_ddl.append(f\"    {col_name} {sql_type}{not_null}{pk}\")\n",
    "    \n",
    "    ddl += \",\\n\".join(colunas_ddl)\n",
    "    ddl += \"\\n);\"\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(ddl_path), exist_ok=True)\n",
    "        with open(ddl_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(ddl)\n",
    "        print(f\"Script DDL salvo com sucesso em: {ddl_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO ao salvar script DDL: {e}\")\n",
    "        \n",
    "    return ddl\n",
    "\n",
    "def executar_ddl_no_banco(ddl_script, env_vars):\n",
    "    print(\"Executando DDL no banco de dados...\")\n",
    "    try:\n",
    "        conn = None\n",
    "        cur = None\n",
    "        conn = psycopg2.connect(\n",
    "            user=env_vars[\"db_user\"], password=env_vars[\"db_password\"],\n",
    "            host=env_vars[\"db_host\"], port=env_vars[\"db_port\"], dbname=env_vars[\"db_name\"]\n",
    "        )\n",
    "        conn.autocommit = True\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(ddl_script)\n",
    "        print(f\"Tabela criada com sucesso no PostgreSQL.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERRO ao executar o script DDL: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if cur: cur.close()\n",
    "        if conn: conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef15433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "=== INICIANDO ETAPA DE CARGA (LOAD) ===\n",
      "==================================================\n",
      "\n",
      "[1/3] Gerando Definição da Tabela (DDL)...\n",
      "Gerando DDL para a tabela 'public.orders'...\n",
      "Script DDL salvo com sucesso em: ../../DataLayer/silver/ddl.sql\n",
      "   -> Script SQL gerado e salvo em: ../../DataLayer/silver/ddl.sql\n",
      "\n",
      "[2/3] Carregando dados no PostgreSQL (public.orders)...\n",
      "Variáveis de ambiente carregadas.\n",
      "Executando DDL no banco de dados...\n",
      "Tabela criada com sucesso no PostgreSQL.\n",
      "   -> Tabela 'public.orders' recriada com sucesso.\n",
      "   -> Inserindo registros...\n",
      "   -> [SUCESSO] Carga no banco de dados concluída.\n",
      "\n",
      "[3/3] Exportando arquivo único CSV...\n",
      "   -> [SUCESSO] Arquivo disponível em: ../../DataLayer/silver/orders.csv\n",
      "\n",
      "==================================================\n",
      "PIPELINE ETL CONCLUÍDO COM SUCESSO\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== INICIANDO ETAPA DE CARGA (LOAD) ===\")\n",
    "print(\"=\"*50)\n",
    "schema_name = \"public\"\n",
    "table_name = \"orders\"\n",
    "ddl_output_path = \"../../DataLayer/silver/ddl.sql\"\n",
    "csv_final_path = f\"../../DataLayer/silver/{table_name}.csv\"\n",
    "csv_temp_path = \"../../DataLayer/silver/temp_csv_output\"\n",
    "\n",
    "print(f\"\\n[1/3] Gerando Definição da Tabela (DDL)...\")\n",
    "try:\n",
    "    ddl_script = gerar_e_salvar_ddl(df_prata_final, table_name, schema_name, ddl_output_path)\n",
    "    print(f\"   -> Script SQL gerado e salvo em: {ddl_output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"   -> [ERRO] Falha ao gerar DDL: {e}\")\n",
    "\n",
    "print(f\"\\n[2/3] Carregando dados no PostgreSQL ({schema_name}.{table_name})...\")\n",
    "env_vars = carregar_variaveis_ambiente()\n",
    "\n",
    "if env_vars:\n",
    "    try:\n",
    "        executar_ddl_no_banco(ddl_script, env_vars)\n",
    "        print(f\"   -> Tabela '{schema_name}.{table_name}' recriada com sucesso.\")\n",
    "        \n",
    "        jdbc_url = f\"jdbc:postgresql://{env_vars['db_host']}:{env_vars['db_port']}/{env_vars['db_name']}\"\n",
    "        jdbc_properties = {\n",
    "            \"user\": env_vars[\"db_user\"], \"password\": env_vars[\"db_password\"],\n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "        \n",
    "        print(f\"   -> Inserindo registros...\")\n",
    "        df_prata_final.write.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=f\"{schema_name}.{table_name}\",\n",
    "            mode=\"overwrite\",\n",
    "            properties=jdbc_properties\n",
    "        )\n",
    "        print(\"   -> [SUCESSO] Carga no banco de dados concluída.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   -> [ERRO CRÍTICO] Falha na carga do banco: {e}\")\n",
    "else:\n",
    "    print(\"   -> [ERRO] Variáveis de ambiente (.env) não encontradas.\")\n",
    "\n",
    "print(f\"\\n[3/3] Exportando arquivo único CSV...\")\n",
    "try:\n",
    "    df_prata_final.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_temp_path)\n",
    "    \n",
    "    arquivo_part = glob.glob(os.path.join(csv_temp_path, \"part-*.csv\"))\n",
    "    \n",
    "    if arquivo_part:\n",
    "        shutil.move(arquivo_part[0], csv_final_path)\n",
    "        shutil.rmtree(csv_temp_path)\n",
    "        print(f\"   -> [SUCESSO] Arquivo disponível em: {csv_final_path}\")\n",
    "    else:\n",
    "        print(\"   -> [ERRO] Arquivo temporário não encontrado.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   -> [ERRO] Falha na exportação CSV: {e}\")\n",
    "\n",
    "caminho_parquet = \"../../DataLayer/silver/data_parquet\"\n",
    "if os.path.exists(caminho_parquet):\n",
    "    shutil.rmtree(caminho_parquet)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PIPELINE ETL CONCLUÍDO COM SUCESSO\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca1dca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Sessão Spark finalizada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
